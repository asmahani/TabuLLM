\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,lmodern}

%% use PythonTeX package for running Python code
\usepackage{pythontex}

%% additional package (optional)
\usepackage{framed}

\usepackage{ulem} % for strikeout text

%% custom commands for formatting
%\newcommand{\pkg}[1]{\texttt{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\fct}[1]{\texttt{#1()}}

%% -- Article metainformation (author, title, ...) -----------------------------

\author{Alireza S. Mahani~\orcidlink{0000-0002-7932-6681}\\Statman Solution Ltd.}
\Plainauthor{Alireza S. Mahani}

\title{\pkg{TabuLLM}: Feature Extraction from Tabular Data Text using Large Language Models (LLMs)}
\Plaintitle{TabuLLM: Feature Extraction from Tabular Data Text using Large Language Models (LLMs)}
\Shorttitle{Feature Extraction using LLMs}

\Abstract{
TBD
}

\Keywords{Python, PythonTeX, code demo, package}
\Plainkeywords{Python, PythonTeX, code demo, package}

\Address{
  Alireza S. Mahani\\
  Statman Solution Ltd.\\
  London, UK\\
  E-mail: \email{statman@statmansolution.com}%\\
  %URL: \url{https://yourwebsite.com/}
}

\begin{document}

%% -- Introduction -------------------------------------------------------------
\section[Introduction]{Introduction} \label{sec:intro}

Text embedding converts natural-language text (a single word or an entire document) to a numeric vector, such that semantically similar texts are mapped to nearby points in the numeric vector space. Embeddings are often used in applications such as information retrieval, search, text classification (including sentiment analysis), and recommender systems. Embeddings can also be used as features in predictive models, an application that hasn't received as much attention. Recently, it has been shown that modern embedding LLMs can exceed domain experts in predictive accuracy for highly-specialized domains \citep{sharabiani2024genai}. 

The \pkg{TabuLLM} Python package provides various functionalities to support the use of LLMs for extracting numeric features from text columns in tabular data, and for incoporating these embeddings in predictive models alongside other features. In particular, \pkg{TabuLLM} consists of three modules: embed, cluster, and explain:
\begin{enumerate}
  \item \textbf{Embed} - A unified interface for converting one or more text column(s) in the data to a numeric matrix, using commercial LLMs (OpenAI, Google Vertex AI), open-source LLMs (available on the Hugging Face model respository, and accessed via the sentence transformers package), as well as earlier-generation embedding methods such as Doc2Vec.
  \item \textbf{Cluster} - Python implementation of spherical k-means for clustering the embedding vectors produced by LLMs. Since embeddings only contain directional information and their magnitude is not meaningful, it is more appropriate to use spherical k-means, which replaces the Euclidean distance - used in standard k-means - with cosine distance.
  \item \textbf{Explain} - 1) Prompt generation for soliciting descriptive labels for data clusters (such as those generated from the embedding vectors, as discussed above), 2) Wrapper for interacting with text-completion LLMs (currently: OpenAI and Google).
\end{enumerate}
All three functionalities are implemented according to the scikit-learn API, so they can be used in predictive pipelines with other scikit-learn transformers and estimators.

\section[TabuLLM Modules]{\pkg{TabuLLM} Modules}\label{sec:modules}

\subsection[embed Module]{\code{embed} Module}\label{subsec:module-embed}

The workhorse of this module is the \code{TextColumnTransformer} class, which implements scikit-learn's transformer interface, namely the \code{fit}, \code{transform} and \code{fit\_transform} methods. The most important argument to the class constructor is \code{model\_type}, which specifies the type of embedding algorithm to be used. As of this writing (September 2024), the available options are \code{openai}, \code{google}, \code{st} (open-source Hugging Face models accessed via the \code{sentence-transformers} package), and \code{doc2vec} - via the \code{gensim} Python package \citep{rehurek_lrec}. With the exception of \code{doc2vec}, the remaining models do not train on the data, which means the \code{fit} function is simply a pass-through for \code{openai}, \code{google} and \code{st}.

The class constructor also accepts dictionary arguments for each of the embedding models listed above, \code{openai\_args}, \code{google\_args}, \code{st\_args} and \code{doc2vec\_args}. For commercial models (OpenAI and Google), the dictionaries must included the necessary account credentials. For all models, the specific model name can be set in the dictionary. For example, as of this writing, the embedding models offered by OpenAI are \code{text-embedding-3-small}, \code{text-embedding-3-large} and \code{text-embedding-ada-002}. See Section \ref{subsec:using-embed} for further details.

Since \pkg{TabuLLM} is designed for tabular data, the \code{X} argument supplied to the \code{transform} method should be a pandas DataFrame with one or more text columns. Multiple text columns are concatenated before embedding, using the \code{colsep} argument supplied to the class constructor. Missing values are replaced with an empty string. In Section \ref{sec:usage} we discuss how alternative approaches such as independent embedding of each text column can be implemented.

Correspondingly, the \code{transform} method returns a pandas DataFrame with the embedded text columns. The number of columns in the output DataFrame is equal to the number of dimensions in the embedding space. The column names are generated by appending the dimension number to the text column name, e.g. \code{diagnoses\_0}, \code{diagnoses\_1}, etc. The prefix can be overridden using the \code{return\_cols\_prefix} argument passed to the class constructor.

\subsection[cluster Module]{\code{cluster} Module}\label{subsec:module-cluster}

Modern embedding LLMs produce high-dimensional vectors, which can pose multiple challenges for predictive models. First, the high dimensionality can lead to overfitting, especially when the number of observations is small relative to the number of features. Second, the high dimensionality can make it difficult to interpret the model, as it is hard to visualize or understand the relationships between the features. Third, the high dimensionality can make it computationally expensive to train the model. To address these challenges, it is often necessary to reduce the dimensionality of the embedding vectors before using them as features in a predictive model.

While different approaches to dimensionality reduction are available, with Principal Components Analysis (PCA) being the most common, clustering can be a more effective approach for embedding vectors. This is due to the fact that embeddings only contain directional information and their magnitude is not meaningful and thus project methods such as PCA can distort the relationships between the vectors. Clustering, on the other hand, can group similar vectors together without distorting the relationships between them. In particular, spherical k-means is a clustering algorithm that replaces the Euclidean distance - used in standard k-means - with cosine distance, which is more appropriate for normalized vectors.

The \code{SphericalKMeans} class in the \code{cluster} module implements spherical k-means in consistency with scikit-learn's estimator interface. The parameters passed to the class constructor are similar to those of scikit-learn's \code{KMeans} class, e.g. allowing the user to specify the number of clusters to generate (\code{n\_clusters}), the maximum number of iterations (\code{max\_iter}), and the number of initializations to perform (\code{n\_init}). A few points are worth highlighting about the \code{SphericalKMeans} class:
\begin{itemize}
  \item The training algorithm is Lloyd's algorithm, with a special approach for centroid initialization and empty-centroid resolution. Random initialization of centroids is based on \textit{unique} observations in the input data, which reduces the probability of empty clusters. When an empty cluster is detected, all centroids are re-initialized, and the iteration count is also reset.
  \item The input matrix \code{X} passed to the \code{fit} method is L2-normalized row-wise before training begins. This speeds up the calculation of cosine distances during the training process.
  \item Consistent with the \code{KMeans} class in scikit-learn, the \code{transform} method returns the distance of each vector to each cluster center, while the \code{predict} method returns the cluster labels for the input vectors. This means that if \code{SphericalKMeans} is used as a transformer in a predictive pipeline, the class-distance matrix - which can be considered a soft version of the class labels - is passed onto the next estimator in the pipeline.
  \item Convenience functions \code{fit\_transform} and \code{fit\_predict} are also available, which combine the \code{fit} and \code{transform} or \code{predict} methods, respectively, into a single call.
\end{itemize}

% add a sentence or two about the advantage of clustering over PCA in terms of interpretability
Clustering also enhances interpretability by grouping similar data points together, making it easier to understand the underlying structure of the data. Unlike PCA, which produces linear combinations of the original features, clustering provides discrete groupings that can be more intuitively understood and explained. This is discussed next.

\subsection[explain Module]{\code{explain} Module}\label{subsec:module-explain}

The core idea here is to use text-generating large language models to provide descriptive labels for the clusters generated by the \code{SphericalKMeans} class using the embedding vectors. Since the embedding vectors are, in turn, produced by embedding LLMs, we refer to this explanation process as (text-generating) `AI explaining AI' (embedding).

The \code{explain} module of \pkg{TabuLLM} provides three functions to support this process: \code{generate_prompt}, \code{generate_response} and \code{one_vs_rest}:
\begin{itemize}
  \item \textbf{\code{generate\_prompt}} - Generates a prompt for soliciting descriptive labels for data clusters. The key inputs are the list of texts used to generate the embedding vectors and hence the clusters, along with cluster labels. It returns prompt instructions as well as the actual payload which is the provided text fields grouped according to the provided cluster labels.
  \item \textbf{\code{generate\_response}} - Submits the prompt generated by \code{generate\_prompt} to an LLM (OpenAI or Google). It takes advantage of the structured-output feature of the LLMs, which allows the user to receive a response containing a collection of (group number, group label - or short desctript, and long desxcription). The function then converts this response to a pandas DataFrame.
  \item \textbf{\code{one\_vs\_rest}} - This function compares the average outcome in each cluster against the average in all other clusters. For binary outcomes, it performs Fisher's exact test and for continuous outcomes, it performs a t-test. The function returns a pandas DataFrame with the test statistics and p-values for each cluster.
\end{itemize}

Advanced topics:
\begin{itemize}
  \item Alternative handling of multiple text columns
  \item Chaining the \code{TextColumnTransformer} and \code{SphericalKMeans} classes in a predictive pipeline
  \item Further compressing the embedding vectors into a single feature using cross-validation / target encoding.
  \item Matryoshka embeddings
\end{itemize}

\section[Using TabuLLM]{Using \pkg{TabuLLM}}\label{sec:usage}

In this section, we demonstrate the use of \pkg{TabuLLM} modules through a series of examples, all using a dataset of pediatric cardiac surgery patients.

\subsection{Dataset}\label{subsec:using-dataset}

The data is an excerpt from that introduced in \cite{sharabiani2024genai}, collected during 2019-2021 from pediatric CPB surgeries in the UK. Columns included:
\begin{itemize}
  \item \code{is\_female}: Patient gender (0: male, 1: female).
  \item \texttt{age}: Patient's age at the time of operation (years).
  \item \code{height}: Patient's height at the time of operation (cm).
  \item \code{weight}: Patient's weight at the time of operation (kg).
  \item \code{optime}: Duration of operation (minutes).
  \item \code{diagnoses}: An ordered collection of medical diagnosis codes for the patient (starting with the primary diagnosis), each including a numeric value followed by a text description of the code. Multiple codes are separated by semicolons.
  \item \code{operations}: Similar to \code{diagnoses}, but representing the procedures performed on the patient during CPB.
  \item \code{aki\_severity}: The outcome variable, severity of postoperative acute kidney injury (AKI). This is a binarized version of the KDIGO score \citep{eknoyan2013kdigo} that can take the values 0, 1, 2, 3. The severity score is 0 if the original score is 0 or 1, and 1 if the original score is 2 or 3.
\end{itemize}
We will be using the \code{diagnoses} and \code{operations} columns for text embedding. We begin by loading the data and printing examples of the text columns:
\begin{pyblock}
import pandas as pd
df = pd.read_csv('../../data/raw.csv')
print(f"Example of diagnoses:\n{df['diagnoses'][2]}\n\n")
print(f"Example of operations:\n{df['operations'][2]}\n")
\end{pyblock}
\stdoutpythontex % \ printpythontex

We can see that each cell in the \code{diagnoses} and \code{operations} columns contains a concatenated list of medical codes, each followed by a description. The order of entry for the codes is medically relevant, with primary codes - diagnosis or operation - included first. %Descriptions are standardized, which allows us to use simpler methods for encoding the text, such as 'bag-of-codes' where each code is represented by a binary feature, and thus each observation is represented by a vector of binary values.

\subsection[embed Module]{\code{embed} Module}\label{subsec:using-embed}

\subsubsection{OpenAI}\label{subsubsec:using-embed-openai}

To use the OpenAI embedding models, we must create an OpenAI client object using a valid API key. As illustrated below, it is recommended to store the API key in a secure location, such as a .env file, and load it using the \code{dotenv} package:
\begin{pyblock}
import os
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=openai_api_key)
\end{pyblock}
We can now instantiate and use \code{TextColumnTransformer}. (NOTE: Running the following code would call OpenAI's embedding model and thus incur a small cost.)
\begin{pyblock}
from TabuLLM.embed import TextColumnTransformer
obj = TextColumnTransformer(
  model_type = 'openai',
  openai_args = {
    'client': client, 
    'model': 'text-embedding-3-small'
  }
)
#X = obj.fit_transform(df.loc[:5, ['diagnoses']])
#print(X.shape)
\end{pyblock}
%\stdoutpythontex % \ printpythontex

As we can see in the output, this embedding model returns a vector of length 1536.

\subsubsection{Google}\label{subsubsec:using-embed-google}

To use Google's embedding models, we must have a valid Google Cloud Platform account, including a Vertex AI project id and location. These are passed by the \code{transform} method of \code{TextColumnTransformer} as \code{project} and \code{location} parameters to the \code{vertexai.init} function. As with OpenAI, these can be loaded as environment variables:
\begin{pyblock}
google_project_id = os.getenv('VERTEXAI_PROJECT')
google_location = os.getenv('VERTEXAI_LOCATION')
#print(f"Google project id: {google_project_id}, location: {google_location}")
\end{pyblock}
%\stdoutpythontex % \ printpythontex

As with OpenAI, the \code{model} parameter can also be passed to override the default value of \code{text-embedding-004}. In addition, two more parameters can be set. First is \code{task} which dictates the type of embedding task used - by Google - to fine-tune model. The default value is \code{SEMANTIC_SIMILARITY}. Second is \code{batch\_size} which specifies the number of text samples to be processed in each batch. The latter is due to a limit imposed by the Google Vertex API. Currently, the limit is 250 samples per batch for the \code{us-central1} region, and 5 elsewhere.

\begin{pyblock}
obj = TextColumnTransformer(
    model_type = 'google'
    , google_args = {
        'project_id': google_project_id
        , 'location': google_location
        , 'model': 'text-embedding-004'
        , 'task': 'SEMANTIC_SIMILARITY'
        , 'batch_size': 250
    }
)
#X = obj.fit_transform(df.loc[:5, ['diagnoses']])
#print(X.shape)
\end{pyblock}
%\stdoutpythontex % \ printpythontex

We see that the Google embedding model returns a vector of length 768.

\subsubsection{Sentence Transformers}\label{subsubsec:using-embed-st}

Besides the commercial embedding models provided by OpenAI and Google, we can also use the open-source LLMs hosted on the Hugging Face platform. To do so, we must use \code{st} for \code{model\_type}, and pass the model name to the \code{st\_args} dictionary. The model name can be found by browsing to the model's homepage on Hugging Face. Below is an example call:
\begin{pyblock}
obj = TextColumnTransformer(
    model_type = 'st'
    , st_args = {
        'model': 'sentence-transformers/all-MiniLM-L6-v2'
    }
)
X = obj.fit_transform(df.loc[:, ['diagnoses']])
print(X.shape)
\end{pyblock}
\stdoutpythontex % \ printpythontex

It is important to note that, unlike the case with OpenAI and Google where inference (text embedding, to be more precise) occurs in the cloud, \code{sentence-transformer} models are downloaded and run locally. This is especially important when using large models with many parameters, since they require relatively large space to store the network weights. Also, performing inference on these networks can be time consuming, unless using accelerators such as GPUs. (The above model, \code{sentence-transformers/all-MiniLM-L6-v2} was chosen specifically for this example since it is a distilled - very small - model.)

An advantage of using sentence transformer models, in addition to being free to use, is that they can be customized for the specific needs of a problem. For instance, one can fine-tune these embedding models on data specific to the application domain. A thorough discussion of fine-tuning, however, is beyond the scope of this tutorial.

\subsubsection{Doc2Vec}\label{subsubsec:using-embed-doc2vec}

[add doc2vec description]

\subsection[cluster Module]{\code{cluster} Module}\label{subsec:using-cluster}

We can now use the embedding vectors produced by the LLMs to cluster the data. We will use the \code{SphericalKMeans} class to cluster the data into 10 groups. The following code demonstrates this:
\begin{pyblock}
from TabuLLM.cluster import SphericalKMeans
cluster = SphericalKMeans(n_clusters=10, n_init=5)
cluster.fit(X)
print(cluster.predict(X[:5]))
\end{pyblock}
\stdoutpythontex % \ printpythontex

Rather than hard clusters, we can also obtain the distance of each vector to each cluster center. This is done by calling the \code{transform} method of the \code{SphericalKMeans} object:
\begin{pyblock}
distances = cluster.transform(X)
print(distances.shape)
\end{pyblock}
\stdoutpythontex % \ printpythontex

These distances can be used as features in a predictive model, as shown later.

The parameter \code{n_init} specifies the number of times the algorithm will be run with different centroid seeds. The final results will be the best output of \code{n\_init} consecutive runs in terms of inertia. Using a higher values of \code{n\_init} will increase the likelihood of finding the global minimum of the inertia function, at the cost of increased computation time.

[we can add code and plot to show the dependence of inertia and cluster consistency - as measured by the rand index - on the number of random starts]

\subsection[explain Module]{\code{explain} Module}\label{subsec:using-explain}

Without any descriptive labels, the clusters generated by the \code{SphericalKMeans} class are difficult to interpret. We can use the \code{explain} module to generate descriptive labels for the clusters by leveraging text-generating LLMs. The core idea is to group the text columns used to generate the embedding vectors according to the cluster labels, and then submit the grouped text to the LLM and solicit descriptive labels for each group. We expect the LLM to examine the similarities of text in each group as well as the differences between groups, and provide descriptive and distinct labels for each group.

\subsubsection{Generate Prompt}\label{subsubsec:using-explain-prompt}

The utility function \code{generate\_prompt} generates a prompt for soliciting descriptive labels for data clusters. The function takes as input the list of texts used to generate the embedding vectors and hence the clusters (\code{text\_list}), along with cluster labels (\code{cluster\_labels}). It returns prompt instructions as well as the actual payload which is the provided text fields grouped according to the provided cluster labels. The class method \code{prep\_X} can be used to prepare the input data for the \code{generate\_prompt} function. This is the same function that is used internally by the class \code{TextColumnTransformer} to prepare the input data for the embedding models.
\begin{pyblock}
from TabuLLM.explain import generate_prompt
prompt, payload = generate_prompt(
    text_list = obj.prep_X(df[['diagnoses']]),
    cluster_labels = cluster.predict(X),
    prompt_observations = 'CPB procedures',
    prompt_texts = 'diagnoses'
)
\end{pyblock}
%\stdoutpythontex % \ printpythontex

We can now explore the auto-generated \code{prompt} and edit it if needed:
\begin{pyblock}
print(prompt)
\end{pyblock}
\stdoutpythontex % \ printpythontex

As can be seen, we are utilizing the structured-output feature of the LLMs, which allows us to receive a response containing a collection of (group number, group label - or short description, and long description). This structure is defined in the \code{MultipleGroupLabels} (internal) class. The function then converts this response to a pandas DataFrame. Currently, OpenAI and Google LLMs are supported.

Let's also examine the \code{payload} by printing the first few lines:
\begin{pyblock}
print('\n'.join(payload.splitlines()[:5]))
\end{pyblock}
\stdoutpythontex % \ printpythontex

\subsubsection{Generate Response}\label{subsubsec:using-explain-response}

The \code{generate\_response} function submits the prompt generated by \code{generate\_prompt} to an LLM (OpenAI or Google). As mentioned, we take advantage of the structured-output feature of the LLMs:
% just display the code here, since we don't want to run it
\begin{pyverbatim}
from TabuLLM.explain import generate_response
explanations = generate_response(
    prompt_instructions = prompt
    , prompt_body = payload
    , model_type = 'openai'
    , openai_client = client
    , openai_model = 'gpt-4o-mini'
)
\end{pyverbatim}

Table \ref{tab:explanations} shows an example of the output produced by OpenAI's \code{gpt-4o} model for the clusters generated by the \code{SphericalKMeans} class.

\begin{table}[!ht]
  \centering
  \begin{tabular}{|p{2cm}|p{4cm}|p{8cm}|}
  \hline
      \textbf{group\_id} & \textbf{description\_short} & \textbf{description\_long} \\ \hline
      0 & Closure of Ventricular Septal Defects (VSDs) & Patients primarily undergoing surgical closure of various types of Ventricular Septal Defects (VSDs), often with associated complications or additional procedural interventions. \\ \hline
      1 & Pulmonary and Tricuspid Valve Surgeries & This group includes patients requiring repairs related to pulmonary and tricuspid valve dysfunctions, including various congenital anomalies and associated complications. \\ \hline
      2 & Tetralogy of Fallot (ToF) Repairs & Patients with Tetralogy of Fallot and related conditions undergoing various surgical repairs impacting right ventricular outflow and pulmonary functions. \\ \hline
      3 & Cardiac Conduit Replacements & Patients with complications related to cardiac conduits, primarily involving replacements due to failures or obstructions. \\ \hline
      4 & Aortic Valve Surgeries & Patients undergoing procedures related to aortic valve dysfunctions, including valvar replacements, repairs, and associated issues in the aortic root. \\ \hline
      5 & Atrioventricular Septal Defects (AVSDs) & Patients primarily undergoing repairs of Atrioventricular Septal Defects, encompassing both complete and partial forms of these congenital heart defects. \\ \hline
      6 & Transposition and Related Surgery & This group involves patients with transposition of the great arteries, often undergoing arterial switch operations or related corrective surgeries. \\ \hline
      7 & Univentricular Heart and Cavopulmonary Connections & Patients requiring complex surgeries for univentricular heart conditions and total cavopulmonary connections, commonly involving conversions and reconnections. \\ \hline
      8 & Atrial Septal Defect (ASD) Repairs & Focuses on surgical repairs of Atrial Septal Defects (ASDs) across various cases, often associated with other cardiac conditions. \\ \hline
      9 & Heart Transplants and Assistance Devices & Patients requiring heart transplantation or mechanical assist devices due to severe cardiomyopathies and associated cardiovascular complications. \\ \hline
  \end{tabular}
  \caption{Descriptive labels - produced by OpenAI's \code{gpt-4o} model - for clusters generated by SphericalKMeans}
  \label{tab:explanations}
\end{table}

\subsubsection{Correlation of Clusters with Outcome}\label{subsubsec:using-explain-one-vs-rest}

The \code{one\_vs\_rest} function compares the average outcome in each cluster against the average in all other clusters. For binary outcomes, it performs Fisher's exact test and for continuous outcomes, it performs a t-test. The function returns a pandas DataFrame with the test statistics and p-values for each cluster. Below is an example call:
\begin{pyblock}
from TabuLLM.explain import one_vs_rest
ovr = one_vs_rest(
    pd.DataFrame({
        'cluster': cluster.predict(X)
        , 'outcome': df['aki_severity']
    })
)
print(ovr)
\end{pyblock}
\stdoutpythontex % \ printpythontex

The above table can be combined with the descriptive labels generated by the LLMs to provide a more comprehensive understanding of the data:
\begin{pyblock}
explanations = pd.read_csv('../../data/explanations.csv')
ovr_plus = pd.concat([explanations[['description_short']].rename(columns = {'description_short': 'group'}), ovr], axis=1)
\end{pyblock}

\subsection[Advanced Topics]{Advanced Topics}\label{subsec:using-advanced}

\subsubsection[Multiple Text Columns]{Multiple Text Columns}\label{subsubsec:using-multiple-columns}

When the first argument, \code{X}, supplied to the \code{fit} method of \code{TextColumnTransformer} is a DataFrame with multiple text columns, the default behavior is to concatenate the columns before embedding. This is done by prepending each column with its corresponding them, and joining the strings using the \code{colsep} argument, which defaults to the string `||'. The actual list of strings sent to the LLM for embedding can be seen by calling the \code{prep\_X} function:
\begin{pyblock}
X = obj.prep_X(df.loc[[2], ['diagnoses', 'operations']])
print(X)
\end{pyblock}
\stdoutpythontex % \ printpythontex

Users can also pre-combine the text columns before passing them to the \code{fit} method. For examples:
\begin{pyblock}
df['diagnoses_and_operations'] = 'Patient diagnoses: ' + df['diagnoses'] + \ 
  ' || CPB procedures: ' + df['operations']
print(df.loc[2, 'diagnoses_and_operations'])
\end{pyblock}
\stdoutpythontex % \ printpythontex

Besides concatenation, it is also possible to embed each text column independently, and then concatenate the resulting embedding vectors. [we can provide example of how to construct a pipeline in scikit-learn]

\subsubsection[Using TabuLLM in Predictive Pipelines]{Using \pkg{TabuLLM} in Predictive Pipelines}\label{subsubsec:using-predictive-pipelines}

\section[Discussion]{Discussion} \label{sec:discussion}

\bibliography{tabullm}

\end{document}
