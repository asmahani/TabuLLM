\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{orcidlink,lmodern}

%% use PythonTeX package for running Python code
\usepackage{pythontex}

%% additional package (optional)
\usepackage{framed}

%% custom commands for formatting
%\newcommand{\pkg}[1]{\texttt{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\fct}[1]{\texttt{#1()}}

%% -- Article metainformation (author, title, ...) -----------------------------

\author{Alireza S. Mahani~\orcidlink{0000-0002-7932-6681}\\Statman Solution Ltd.}
\Plainauthor{Alireza S. Mahani}

\title{\pkg{TabuLLM}: Feature Extraction from Tabular Data Text using Large Language Models (LLMs)}
\Plaintitle{TabuLLM: Feature Extraction from Tabular Data Text using Large Language Models (LLMs)}
\Shorttitle{Feature Extraction using LLMs}

\Abstract{
TBD
}

\Keywords{Python, PythonTeX, code demo, package}
\Plainkeywords{Python, PythonTeX, code demo, package}

\Address{
  Alireza S. Mahani\\
  Statman Solution Ltd.\\
  London, UK\\
  E-mail: \email{statman@statmansolution.com}%\\
  %URL: \url{https://yourwebsite.com/}
}

\begin{document}

%% -- Introduction -------------------------------------------------------------
\section[Introduction]{Introduction} \label{sec:intro}

Text embedding converts natural-language text (a single word or an entire document) to a numeric vector, such that semantically similar texts are mapped to nearby points in the numeric vector space. Embeddings are often used in applications such as information retrieval, search, text classification (including sentiment analysis), and recommender systems. Embeddings can also be used as features in predictive models, an application that hasn't received as much attention. Recently, it has been shown that modern embedding LLMs can exceed domain experts in predictive accuracy for highly-specialized domains \citep{sharabiani2024genai}. 

The \pkg{TabuLLM} Python package provides various functionalities to support the use of LLMs for extracting numeric features from text columns in tabular data, and for incoporating these embeddings in predictive models alongside other features. In particular, \pkg{TabuLLM} consists of three modules: embed, cluster, and explain:
\begin{enumerate}
  \item \textbf{Embed} - A unified interface for converting one or more text column(s) in the data to a numeric matrix, using commercial LLMs (OpenAI, Google Vertex AI), open-source LLMs (available on the Hugging Face model respository, and accessed via the sentence transformers package), as well as earlier-generation embedding methods such as Doc2Vec.
  \item \textbf{Cluster} - Python implementation of spherical k-means for clustering the embedding vectors produced by LLMs. Since embeddings only contain directional information and their magnitude is not meaningful, it is more appropriate to use spherical k-means, which replaces the Euclidean distance - used in standard k-means - with cosine distance.
  \item \textbf{Explain} - 1) Prompt generation for soliciting descriptive labels for data clusters (such as those generated from the embedding vectors, as discussed above), 2) Wrapper for interacting with text-completion LLMs (currently: OpenAI and Google).
\end{enumerate}
All three functionalities are implemented according to the scikit-learn API, so they can be used in predictive pipelines with other scikit-learn transformers and estimators.

\section[TabuLLM Modules]{\pkg{TabuLLM} Modules}\label{sec:modules}

\section[Using TabuLLM]{Using \pkg{TabuLLM}}\label{sec:usage}

In this section, we demonstrate the use of \pkg{TabuLLM} modules through a series of examples, all using a dataset of pediatric cardiac surgery patients.

\subsection{Dataset}\label{subsec:dataset}

The data is an excerpt from that introduced in \cite{sharabiani2024genai}, collected during 2019-2021 from pediatric CPB surgeries in the UK. Columns included:
\begin{itemize}
  \item \code{is\_female}: Patient gender (0: male, 1: female).
  \item \texttt{age}: Patient's age at the time of operation (years).
  \item \code{height}: Patient's height at the time of operation (cm).
  \item \code{weight}: Patient's weight at the time of operation (kg).
  \item \code{optime}: Duration of operation (minutes).
  \item \code{diagnoses}: An ordered collection of medical diagnosis codes for the patient (starting with the primary diagnosis), each including a numeric value followed by a text description of the code. Multiple codes are separated by semicolons.
  \item \code{operations}: Similar to \code{diagnoses}, but representing the procedures performed on the patient during CPB.
  \item \code{aki\_severity}: Severity of postoperative acute kidney injury (AKI). This is a binarized version of the KDIGO score \citep{eknoyan2013kdigo} that can take the values 0, 1, 2, 3. The severity score is 0 if the original score is 0 or 1, and 1 if the original score is 2 or 3.
\end{itemize}
We will be using the \code{diagnoses} and \code{operations} columns for text embedding. Let's load and explore the data:
\begin{pyblock}
import pandas as pd
df = pd.read_csv('../../data/raw.csv')
df.columns = ['isFemale'] + df.columns[1:].tolist()
print(df.head().to_latex(escape=False))
\end{pyblock}
\stdoutpythontex % \ printpythontex

\section[Discussion]{Discussion} \label{sec:discussion}

\bibliography{tabullm}

\end{document}
