{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Cluster` Module in `TabuLLM`: Spherical K-Means for L2-Normalized Text Embeddings\n",
    "\n",
    "## Why Clustering?\n",
    "\n",
    "A common complain about embeddings is their lack of explainability, which is part of the broader 'black-box' narrative about ML/AI algorithms. This is especially acute given that embedding vectors produced by modern LLMs are often high-dimensional. For instance, OpenAI's `text-embedding-3-large` outputs a vector of length 3072! In many predictive applications, including a such high-dimensional vector would lead to several issues. First is that the vector consumes too many degrees of freedom and including the full vector may be impractical, especially for small datasets. Secondly, interpreting - or explaining - the resulting model would be challenging.\n",
    "\n",
    "One way to validate or 'explain' the embeddings is to use them to cluster the observations, and examine the resulting clusters. Cluster labels can also be used as a categorical feature in downstream predictive models. As such, clusterng can server as a dimensionality-reduction technique. These are the motivations behind the `cluster` and `explain` modules of `TabuLLM`.\n",
    "\n",
    "## Why *Spherical* K-Means?\n",
    "\n",
    "A useful clustering technique that is often used with text embeddings is k-means. In its standard form, k-means uses the Euclidean - or L2 norm - to measure the distance between a pair of vectors. This is is how data points are assigned to cluster centroids in the Lloyd's algorithm for training k-means. Correspondingly, in the centroid-update step, the mean of vectors mapped to each centroid are calculated to update the centroid vector. The mean vector minimizes the total L2 distance from all cluster members.\n",
    "\n",
    "Text embeddings, however, only contain directional information, and no magnitude. This means they are either L2 normalized, or must be L2 normalized by the user. This can be seen by examining the output of sentence transformer models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\anaconda3\\envs\\devTEFE\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      1.0\n",
      "      ... \n",
      "825    1.0\n",
      "826    1.0\n",
      "827    1.0\n",
      "828    1.0\n",
      "829    1.0\n",
      "Length: 830, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "from TabuLLM.embed import TextColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/raw.csv')\n",
    "\n",
    "X = TextColumnTransformer(\n",
    "    model_type = 'st'\n",
    ").fit_transform(df[['diagnoses']])\n",
    "#print((X**2).sum(axis=1)) # should be all 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above property, we propose that the distance metric used for applying K-means to L2-normalized embedding vectors must be the cosine distance, rather than L2 norm. It must be noted that, during the assignment step of Lloyd's algorithm, cosine and L2 distances would produce identical outcomes, since they would rank any collectiond of pairs of points the same way.\n",
    "\n",
    "In the centroid update step, the cosine-distance approach would take the mean of all cluster members and apply an L2 normalization. This last step creates a divergence between standard k-means and spherical k-means.\n",
    "\n",
    "## The `SphericalKMeans` Class\n",
    "\n",
    "The `SphericalKMeans` class in `TabuLLM` implements the familiar interface of `KMeans` in the `scikit-learn` package, including class constructor arguments and public methods. Besides the key difference with standard k-means that is using cosine distance instead of L2 distance, there are a few differences in the implementation of `SphericalKMeans` that are worth highlighting:\n",
    "1. Applying unique to rows of X before random subset selection (for initialization)\n",
    "1. Handling of empty clusters\n",
    "\n",
    "Other topics to comment on:\n",
    "- Speed vs. standard k-means\n",
    "- Proof of superiority (or lack thereof) in clustering embeddings\n",
    "- Acknowledge that there are other clustering techniques besides k-means, perhaps argue why k-means is still a better choice than seemingly more advanced techniques.\n",
    "- Discuss the nuanced difference between fit_predict and fit_transform\n",
    "\n",
    "### Cluster Consistency and Sensitivity to Initialization\n",
    "\n",
    "A major concern with kmeans - or other clustering techniques that require some form of cluster initialization - is that the final results are sensitive to the starting point. Similar to scikit-learn's `KMeans`, `SphericalKMeans` allows users to perform multiple runs with different random initializations (via `n_init`), and use the best result, i.e., one with lowest total within-cluster distance (aka *inertia*).\n",
    "\n",
    "It will be interesting to study how changing `n_init` impacts cluster stability. To quantify the latter, we use two metric, adjusted rand index (ARI) and adjusted mutual information (AMI), both available in scikit-learn. We calculate ARI and AMI between multiple runs of `SphericalKMeans`, and while changing `n_init` from one set of runs to the next. We hope to see that as `n_init` is increased, so does the average ARI/AMI values amongst pairs of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init = 1\n",
      "ARI = 0.5794053807032611, AMI = 0.592852688790101\n",
      "n_init = 3\n",
      "ARI = 0.6350779645549716, AMI = 0.6499854324630371\n",
      "n_init = 10\n",
      "ARI = 0.7423067530123156, AMI = 0.759498012313953\n",
      "n_init = 30\n",
      "ARI = 0.774646284122888, AMI = 0.7921260431790932\n",
      "n_init = 100\n",
      "ARI = 0.7546001383487387, AMI = 0.7710290978101386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from TabuLLM.cluster import SphericalKMeans\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "n_init_list = [1, 3, 10, 30, 100]\n",
    "nrun = 10\n",
    "ari_vec = np.zeros(len(n_init_list))\n",
    "ami_vec = np.zeros(len(n_init_list))\n",
    "for idx, n_init in enumerate(n_init_list):\n",
    "    print(f'n_init = {n_init}')\n",
    "    # initialize a numpy matrix to hold all X's\n",
    "    labels_all = np.zeros((df.shape[0], nrun))\n",
    "    for i in range(nrun):\n",
    "        labels = SphericalKMeans(n_clusters=10, n_init=n_init).fit_predict(X)\n",
    "        labels_all[:, i] = labels\n",
    "    # calculate adjusted rand index between all pairs of columns in X_all\n",
    "    ari = 0.0\n",
    "    ami = 0.0\n",
    "    for i in range(nrun):\n",
    "        for j in range(i+1, nrun):\n",
    "            ari = ari + adjusted_rand_score(labels_all[:, i], labels_all[:, j])\n",
    "            ami = ari + adjusted_mutual_info_score(labels_all[:, i], labels_all[:, j])\n",
    "    ari = ari / (nrun * (nrun - 1) / 2)\n",
    "    ari_vec[idx] = ari\n",
    "    ami = ami / (nrun * (nrun - 1) / 2)\n",
    "    ami_vec[idx] = ami\n",
    "\n",
    "    print(f'ARI = {ari}, AMI = {ami}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, somewhere between 30 and 100 initializations is likely to be sufficient to achieve maximum cluster stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devTEFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
