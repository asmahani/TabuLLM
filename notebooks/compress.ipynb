{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting a Single Feature Column from Matrix of Embeddings: The `Compress` Module in `TabuLLM`\n",
    "\n",
    "## Overview\n",
    "\n",
    "As we saw in previous tutorials, modern text embedding LLMs produce high-dimensional vectors. For example, OpenAI's `text-embedding-3-large` outputs embedding vectors of length 3072. We also mentioned that clustering can be used as a dimensionality-reduction approach, where the cluster labels are treated as a categorical variable.\n",
    "\n",
    "There is, however, an even more efficient way of distilling the information captured in the embeddings into a single variable: Train a k-nearest-neighbor classifier (or regressor, depending on type of outcome variable) on embedding vectors, wrapped in cross-fit. Use the predictions of this cross-fit-wrapped KNN model as a single feature in downstream predictive models. Cross-fit ensures that in-sample predictions are close to being out-of-sample prediction, thus minimizing the risk of overfitting.\n",
    "\n",
    "## The `CompressClassifier` and `CompressRegressor` Transformers\n",
    "\n",
    "These classes implement the scikit-learn transformer interface and can therefore be included in composite pipelines as a feature-extraction step. Let's examine the class constructor for `CompressClassifier`:\n",
    "\n",
    "`\n",
    "CompressClassifier(nx=None, ncv=5, logit=True, laplace=True, **kwargs)\n",
    "`\n",
    "\n",
    "Below is an explanation of the constructor arguments:\n",
    "- `nx`: This determines how many elements of the embedding vector - starting at index 0 - to include in the compression algorithm. The default value is `none`, which means the entire length of the embedding vectors will be used.\n",
    "- `ncv`: This determined the cross-fit strategy. If an integer, it sets the number of folds. If an object of class `sklearn.model_selection.KFold`, it will be used directly in the cross-fit algorithm.\n",
    "- `logit`: This boolean decides whether the logit of predicted class probabilities should be returned as the extracted feature or not. Default is `True`.\n",
    "- `laplace`: This boolean determines whether Laplace smoothing should be applied to the predicted probabilities of the KNN model. Default is `True`.\n",
    "- `**kwargs`: Keyword arguments passed to the class constructor for `sklearn.neighbors.KNeighborsClassifier`. The most important one is `n_neighbors`, which sets the number of nearest neighbors used to calculate the predicted class probability for each prediction.\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's revisit the AKI problem, and use `CompressClassifier` to generate a risk score from our text column. We begin by using a small LLM from Hugging Face to generate the text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alire\\anaconda3\\envs\\devTEFE\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\alire\\anaconda3\\envs\\devTEFE\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (830, 384)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from TabuLLM.embed import TextColumnTransformer\n",
    "df = pd.read_csv('../data/raw.csv')\n",
    "embeddings = TextColumnTransformer(\n",
    "    type = 'st'\n",
    "    , embedding_model_st = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ").fit_transform(df.loc[:, ['diagnoses']])\n",
    "print(f'Shape of embeddings: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create and train an instance of `CompressClassifier` on the embedding matrix and outcome vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of risk_score: (830, 1)\n"
     ]
    }
   ],
   "source": [
    "from TabuLLM.compress import CompressClassifier\n",
    "risk_score = CompressClassifier(\n",
    "    nx = 100\n",
    "    , ncv = 5\n",
    "    , logit = True\n",
    "    , laplace = True\n",
    "    , n_neighbors = 50\n",
    ").fit_transform(embeddings, df['aki_severity'])\n",
    "print(f'Shape of risk_score: {risk_score.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the risk score is a single column of numbers including both positive and negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min/Max of risk score: -3.2188758248682006 / 0.47000362924573574\n"
     ]
    }
   ],
   "source": [
    "# range of risk score\n",
    "print(f'Min/Max of risk score: {risk_score.min()} / {risk_score.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of negative values is due to the flag `logit` being `True`. If we flip this to `False', we obtain probabilities instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min/Max of risk score prob: 0.038461538461538464 / 0.5961538461538461\n"
     ]
    }
   ],
   "source": [
    "risk_score_prob = CompressClassifier(\n",
    "    nx = 100\n",
    "    , ncv = 5\n",
    "    , logit = False\n",
    "    , laplace = True\n",
    "    , n_neighbors = 50\n",
    ").fit_transform(embeddings, df['aki_severity'])\n",
    "print(f'Min/Max of risk score prob: {risk_score_prob.min()} / {risk_score_prob.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it All Together: Using a Text Column in a Predictive Model\n",
    "\n",
    "Recall that our ultimate goal in `TabuLLM` is to incorporate text columns into predictive models by taking advantage of modern LLMs. Let's use the AKI dataset to illustrate how the `CompressClassifier` transformer can be included in a predictive pipeline, where the risk score extracted from the text column `diagnoses` is included alongside other patient attributes in a logistic regression model to predict severe postoperative AKI.\n",
    "\n",
    "To start, we prepare our data, i.e., the feature matrix and the response vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_baseline = ['is_female', 'age', 'height', 'weight', 'optime']\n",
    "features_embedding = [f'X_{i}' for i in range(embeddings.shape[1])]\n",
    "X = pd.concat([embeddings, df[features_baseline]], axis = 1)\n",
    "y = df['aki_severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our baseline and embedding pipelines. In the baseline case, we simply drop the embedding features and pass the baseline features to a logistic regression model. In the embedding case, we apply `CompressClassifier` to the embedding features, and then add them to the baseline features before applying logistic regression. For KNN inside `CompressClassifier`, we use 50 neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ct_baseline = ColumnTransformer([\n",
    "    ('baseline', 'passthrough', features_baseline)\n",
    "], remainder = 'drop')\n",
    "pipeline_baseline = Pipeline([\n",
    "    ('coltrans', ct_baseline)\n",
    "    , ('logit', LogisticRegression(penalty = None, solver = 'newton-cholesky', max_iter = 1000))\n",
    "])\n",
    "\n",
    "ct_embedding = ColumnTransformer([\n",
    "    ('baseline', 'passthrough', features_baseline)\n",
    "    , ('embedding', CompressClassifier(n_neighbors = 50), features_embedding)\n",
    "], remainder = 'drop')\n",
    "pipeline_embedding = Pipeline([\n",
    "    ('coltrans', ct_embedding)\n",
    "    , ('logit', LogisticRegression(penalty = None, solver = 'newton-cholesky', max_iter = 1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a `KFold` object and pass it alongside the above two pipelines to sklearn's `cross_val_score` to obtain the area under ROC for each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_baseline.fit(X, y)\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits = 50, shuffle = True, random_state = 1234)\n",
    "\n",
    "auc_baseline = cross_val_score(\n",
    "    pipeline_baseline\n",
    "    , X, y, cv = kf\n",
    "    , scoring = 'roc_auc'\n",
    "    #, n_jobs = self.n_jobs\n",
    "    #, verbose = verbose\n",
    ")\n",
    "\n",
    "auc_embedding = cross_val_score(\n",
    "    pipeline_embedding\n",
    "    , X, y, cv = kf\n",
    "    , scoring = 'roc_auc'\n",
    "    #, n_jobs = self.n_jobs\n",
    "    #, verbose = verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform paired t-test to compare the fold-level AUC scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=-2.6262994278320235, pvalue=0.011486618914033459, df=49)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paires t-test\n",
    "from scipy.stats import ttest_rel\n",
    "ttest_rel(auc_baseline, auc_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, at the 5\\% significance level, adding the text column via embedding + compression has improved AUC compared to the baseline model. Keep in mind that the LLM we used in this tutorial was a small one that is likely to be inferior to the most cutting-edge models from OpenAI and Google. For a more thorough study of text embeddings in the AKI dataset, see Sharabiani et al (2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devTEFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
